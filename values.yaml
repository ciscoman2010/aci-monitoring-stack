image:
  repository: quay.io/camillo/aci-exporter
  tag: 58
  pullPolicy: Always
config:
  port: 9643
  url: aci-exporter-svc

  # Helm can't use variables in a Value file so naming the service 
  # Dynamically (i.e. including the release name) will break the prometheus sub-chart
  # The easiest solution (and the only one I found that does not involves patching the prom sub-chart)
  # is to use anchors so we can define the http_service_discovery URL needed by prometheus 
  # and then pass it in the prometheus.serverFiles.prometheus.yml.scrape_configs.http_sd_configs.url paramater
  # Sadly acnhors don't support string concatenation so we have to ensure the aci-sd-url matches the config.port and config.url
  # You  need to change this ONLY if you want to deploy this chart multiple times in the same namespace to avoid 
  # having duplicate service name in the same NS.
  aci-sd-url: &aci-sd-url http://aci-exporter-svc:9643/sd
  prefix: aci_
  httpclient:
    insecurehttps: true
    keepalive: 120
    timeout: 30


prometheus:
  # We are deploying a dedicated Prometheus instance for the aci-exporter
  # So we don't care about collecting metrics from the K8s cluster itself
  kube-state-metrics:
    enabled: false
  prometheus-node-exporter:
    enabled: false
  prometheus-pushgateway:
    enabled: false
  server:
    prefixURL: ""
  serverFiles:
    prometheus.yml:
      rule_files:
        - /etc/config/recording_rules.yml
        - /etc/config/alerting_rules.yml
      scrape_configs:
        - job_name: prometheus
          static_configs:
            - targets:
              - localhost:9090
        - job_name: aci-exporter-sd-apics
          scrape_interval: 1m
          scrape_timeout: 30s
          metrics_path: /probe
          params:
              # List of the queries to execute on the fabric level. They need to match the aci-exporter config
              # DO NOT INSERT SPACES and use \ for next line or aci-exporter will not be able to parse the queries
            queries:
              - "health,fabric_node_info,max_capacity,max_global_pctags,\
                vlans,static_binding_info,dynamic_binding_info,node_count,\
                epg_port_vlan_binding,epg_port_vxlan_binding,object_count,\
                ps_power_usage,apic_hw_sensors,uptime_topsystem"
          scheme: http
          http_sd_configs:
            - url: *aci-sd-url
              refresh_interval: 5m
          relabel_configs:
            - source_labels: [ __meta_role ]
              # This config executes the queries at the "fabric" level and is used to probe any of the APICs 
              # to get metrics about all the devices in the fabric. A classic use case is to get for example the vlan pools
              # the status of the nodes or the scale profile for the whole fabric etc... 
              regex: "aci_exporter_fabric"
              action: "keep"
            - source_labels: [ __address__ ]
              target_label: __param_target
            - source_labels: [ __param_target ]
              target_label: instance
            - source_labels: [ __meta_url ] 
              regex: https?://(.*)/.*
              replacement: "$1"
              target_label: __address__     
        - job_name: aci-exporter-sd-switches
          scrape_interval: 1m
          scrape_timeout: 30s
          metrics_path: /probe
          params:
              # List of the queries to execute on the fabric level. They need to match the aci-exporter config
              # DO NOT INSERT SPACES and use \ for next line or aci-exporter will not be able to parse the queries
            queries:
              - "node_bgp_peers,node_bgp_peers_af,node_interface_info,\
                node_interface_rx_stats,node_interface_rx_err_stats,\
                node_interface_tx_stats,node_interface_tx_err_stats,node_cpu,node_memory,\
                node_scale_profiles,node_active_scale_profile,node_tcam_current,\
                node_labels_current,node_mac_current,node_ipv4_current,\
                node_ipv6_current,node_mcast_current,node_vlan_current,\
                node_lpm_current,node_slash32_current,node_slash128_current,\
                node_scale_ctx,node_ospf_neighbors,node_fru_power_usage,node_temperature"
          scheme: http
          http_sd_configs:
            - url:  *aci-sd-url
              refresh_interval: 5m
          relabel_configs:
            - source_labels: [ __meta_role ]
              # Include Only the Switches, this is used to execute the queries on all the switches in the fabric
              # but not on the APIC, for example the APIC have no TCAM so we need to exclude them.
              regex: "(spine|leaf)"
              action: "keep"

            # Get the target (Fabric Name) param from __address__ that is <fabric>#<oobMgmtAddr> by default
            - source_labels: [ __address__ ]
              separator: "#"
              regex: (.*)#(.*)
              replacement: "$1"
              target_label: __param_target
            
            # Get the node Address param from __address__ that is <fabric>#<oobMgmtAddr> by default
            - source_labels: [ __address__ ]
              separator: "#"
              regex: (.*)#(.*)
              replacement: "$2"
              target_label: __param_node
            # Get the aci-exporter URL from the service discovery URL
            - source_labels: [ __meta_url ]
              regex: https?://(.*)/.*
              replacement: "$1"
              target_label: __address__

            # Set instance to the ip/hostname from the __param_node
            - source_labels: [ __param_node ]
              target_label: instance

            # Add labels from discovery
            - source_labels: [ __meta_fabricDomain ]
              target_label: aci
            - source_labels: [ __meta_id ]
              target_label: nodeid
            - source_labels: [ __meta_podId ]
              target_label: podid
            - source_labels: [ __meta_role ]
              target_label: role
            - source_labels: [ __meta_name ]
              target_label: name   
  service:
    servicePort: 80
  alertmanager:
    enabled: true
    service:
      port: 9093
grafana:
  enabled: true
  defaultDashboardsEnabled: false
  sidecar:
    datasources:
      name: Prometheus
      uid: prometheus
      enabled: true
      defaultDatasourceEnabled: true
      isDefaultDatasource: true
      label: grafana_datasource
      labelValue: "aci-monitoring-stack"
      alertmanager:
        enabled: true
        name: Alertmanager
        uid: alertmanager
        handleGrafanaManagedAlerts: false
        implementation: prometheus
    dashboards:
      enabled: true
      label: grafana_dashboard
      labelValue: "aci-monitoring-stack"
      # This is needed to place the dashboard inside the folder specified
      # in the `k8s-sidecar-target-directory` annotation in the ConfigMap
      provider:
        foldersFromFilesStructure: true
    alerts:
      enabled: true
      label: grafana_alert
      labelValue: "aci-monitoring-stack"

promtail:
  enabled: true
  daemonset:
    enabled: false
  deployment:
    enabled: true
  replicaCount: 1
  #Only used to ingest external logs from ACI so no need to mount any of the local volumes
  defaultVolumes: []
  defaultVolumeMounts: []
  podSecurityPolicy:
    privileged: fasle
    allowPrivilegeEscalation: false
    volumes:
      - 'secret'
      - 'downwardAPI'
  config:
    clients:
    - url: http://loki-gateway/loki/api/v1/push
    snippets:
      scrapeConfigs: |
        - job_name: syslog
          syslog:
            listen_address: 0.0.0.0:1514
            labels:
              job: aci-monitoring-stack
          relabel_configs:
            - source_labels:
                - __syslog_message_hostname
              target_label: host



loki:
  enabled: true
  fullnameOverride: "loki"
  monitoring:
    dashboards:
      enabled: false
    rules:
      enabled: false
    serviceMonitor:
      enabled: false
    selfMonitoring:
      enabled: false
      grafanaAgent:
        installOperator: false
    lokiCanary:
      enabled: false
  lokiCanary:
    enabled: false
  test:
    enabled: false
  loki:
    enabled: true
    auth_enabled: false
    gateway:
      service:
        port: 80
    datasource:
      jsonData: "{}"
      uid: ""
    commonConfig:
      replication_factor: 1
    schemaConfig:
      configs:
        - from: 2024-04-01
          store: tsdb
          object_store: s3
          schema: v13
          index:
            prefix: loki_index_
            period: 24h
    ingester:
      chunk_encoding: snappy
    tracing:
      enabled: false
    querier:
      # Default is 4, if you have enough memory and CPU you can increase, reduce if OOMing
      max_concurrent: 4
  deploymentMode: SimpleScalable
  backend:
    replicas: 3
  read:
    replicas: 3
  write:
    replicas: 3
  # Enable minio for storage
  minio:
    enabled: true
  singleBinary:
    replicas: 0

  ingester:
    replicas: 0
  querier:
    replicas: 0
  queryFrontend:
    replicas: 0
  queryScheduler:
    replicas: 0
  distributor:
    replicas: 0
  compactor:
    replicas: 0
  indexGateway:
    replicas: 0
  bloomCompactor:
    replicas: 0
  bloomGateway:
    replicas: 0